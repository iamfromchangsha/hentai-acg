import requests
import cloudscraper
from bs4 import BeautifulSoup
import os
from pathlib import Path
import re
from urllib.parse import urlparse, urljoin
import time
import random
import urllib3
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


REMOTE_SERVER_URL = "http://192.168.1.103:5066/"  # æ›¿æ¢ä¸ºå®é™…IPæˆ–åŸŸå

# åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯
def create_session():
    # ä½¿ç”¨cloudscraperåˆ›å»ºä¼šè¯ï¼Œå®ƒå·²ç»å†…ç½®äº†ååçˆ¬è™«åŠŸèƒ½
    session = cloudscraper.create_scraper()
    return session

# ç”¨æˆ·ä»£ç†åˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
]

def download_cos_photos():
    # åˆ›å»ºä¼šè¯
    session = create_session()
    
    # åˆ›å»ºåŸºç¡€ç›®å½•
    base_dir = Path("")
    base_dir.mkdir(parents=True, exist_ok=True)
    
    shuliang = int(input("è¯·è¾“å…¥èµ·å§‹é¡µé¢æ•°: "))
    while True:
        cosurl = get_cos_url(shuliang)
        print(f"\næ­£åœ¨å¤„ç†é¡µé¢: {cosurl}")
        shuliang += 1
        
        try:
            results = parse_cos_list(cosurl, session)
            if not results:
                print(f"é¡µé¢ {shuliang-1} æœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡")
                continue
                
            print(f"æ‰¾åˆ° {len(results)} ä¸ªCOSä½œå“")
            
            for href, title in results:
                if not is_valid_cos_item(href, title):
                    continue
                    
                print(f"\nå¼€å§‹å¤„ç†ä½œå“: {title}")
                
                # ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜åï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦
                clean_title = clean_filename(title)
                
                cos_page_url = "https://acgmhn.com" + href
                max_page = get_max_page_number(cos_page_url, session)
                
                if max_page is None:
                    print(f"æ— æ³•è·å– {title} çš„é¡µæ•°ï¼Œè·³è¿‡")
                    continue
                    
                print(f"æ£€æµ‹åˆ° {title} æœ‰ {max_page} å¼ å›¾ç‰‡")
                download_cos_album(cos_page_url, clean_title, max_page, base_dir, session)
                
        except Exception as e:
            print(f"å¤„ç†é¡µé¢æ—¶å‡ºé”™: {str(e)}")
            continue

def get_cos_url(page_num):
    return f"https://www.acgmhn.com/cos/index-{page_num}.html"

def parse_cos_list(url, session):
    try:
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        response = session.get(url, headers=headers, timeout=10)  # ç§»é™¤äº†verify=Falseå‚æ•°ï¼Œcloudscraperé»˜è®¤ä¸éªŒè¯SSL
        
        if response.status_code != 200:
            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, "html.parser")
        list_items = soup.find_all('a')
        results = []
        
        for item in list_items:   
            href = item.get('href')
            title = item.get('title')
            if href and title:  # ç¡®ä¿ä¸¤è€…éƒ½æœ‰å€¼
                results.append((href, title))
        
        # è¿‡æ»¤æ‰æ— æ•ˆå…ƒç´  - æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
        if len(results) > 49:
            return results[14:-35]
        return results
        
    except Exception as e:
        print(f"è§£æåˆ—è¡¨é¡µé¢å‡ºé”™: {str(e)}")
        return []

def is_valid_cos_item(href, title):
    if not title or title == "None":
        return False
    if '3D' in title:
        return False
    if not href.endswith('.html'):
        return False
    if not re.search(r'\d', href):
        return False
    return True

def get_max_page_number(page_url, session):
    try:
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        response = session.get(page_url, headers=headers, timeout=10)  # ç§»é™¤äº†verify=Falseå‚æ•°
        
        if response.status_code != 200:
            print(f"è·å–é¡µé¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
            
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ›´ç²¾ç¡®åœ°æŸ¥æ‰¾åˆ†é¡µå…ƒç´ 
        pagination = soup.find('div', class_='pagination')
        if pagination:
            page_links = pagination.find_all('a')
            if page_links:
                # è·å–å€’æ•°ç¬¬äºŒä¸ªé“¾æ¥çš„é¡µç ï¼ˆæœ€åä¸€ä¸ªé€šå¸¸æ˜¯"ä¸‹ä¸€é¡µ"ï¼‰
                last_page_link = page_links[-2]
                try:
                    return int(last_page_link.text.strip())
                except ValueError:
                    pass
        
        # å›é€€æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ä¸­çš„æ•°å­—
        a_tags = soup.find_all('a')
        max_number = 0
        
        for tag in a_tags:
            text = tag.get_text().strip()
            if text.isdigit():
                page_num = int(text)
                if page_num > max_number:
                    max_number = page_num
                    
        return max_number if max_number > 0 else None
        
    except Exception as e:
        print(f"è·å–æœ€å¤§é¡µæ•°å‡ºé”™: {str(e)}")
        return None

def get_image_url(page_url, session):
    try:
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        response = session.get(page_url, headers=headers, timeout=10)  # ç§»é™¤äº†verify=Falseå‚æ•°
        
        if response.status_code != 200:
            print(f"è·å–å›¾ç‰‡é¡µé¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
            
        soup = BeautifulSoup(response.text, "html.parser")
        
        # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾å›¾ç‰‡
        img_tag = soup.find('img', class_='lazyload')
        if img_tag:
            data_src = img_tag.get('data-src')
            if data_src:
                # ç¡®ä¿URLå®Œæ•´
                if data_src.startswith('//'):
                    data_src = 'https:' + data_src
                elif data_src.startswith('/'):
                    data_src = 'https://www.hentai-acg.com' + data_src
                return data_src
                
        img_tag = soup.find('img', class_='content-image')
        if img_tag:
            src = img_tag.get('src')
            if src:
                # ç¡®ä¿URLå®Œæ•´
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = 'https://www.hentai-acg.com' + src
                return src
                
        # å°è¯•sourceæ ‡ç­¾
        source_tag = soup.find('source')
        if source_tag:
            srcset = source_tag.get('srcset')
            if srcset:
                # å–ç¬¬ä¸€ä¸ªURL
                src = srcset.split()[0]
                # ç¡®ä¿URLå®Œæ•´
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = 'https://www.hentai-acg.com' + src
                return src
        
        # æœ€åå°è¯•ç¬¬ä¸€ä¸ªimgæ ‡ç­¾
        img_tags = soup.find_all('img')
        if img_tags:
            img_src = img_tags[0].get('src') or img_tags[0].get('data-src')
            if img_src:
                # ç¡®ä¿URLå®Œæ•´
                if img_src.startswith('//'):
                    img_src = 'https:' + img_src
                elif img_src.startswith('/'):
                    img_src = 'https://www.hentai-acg.com' + img_src
                return img_src
                
        return None
        
    except Exception as e:
        print(f"è·å–å›¾ç‰‡URLå‡ºé”™: {str(e)}")
        return None

def get_image_format(url):
    if not url:
        return "jpg"
    
    try:
        path = urlparse(url).path
        if '.' in path:
            extension = path.split('.')[-1].lower()
            # ç§»é™¤å¯èƒ½å­˜åœ¨çš„æŸ¥è¯¢å‚æ•°
            extension = extension.split('?')[0]
            if extension in ['jpg', 'jpeg', 'png', 'gif', 'webp', 'avif']:
                return extension
        return "jpg"
    except:
        return "jpg"

def folder_exists(folder_path):
    return os.path.exists(folder_path) and os.path.isdir(folder_path)

def file_exists(file_path):
    return os.path.isfile(file_path)

def download_cos_album(first_page_url, title, max_page, base_dir, session):
    print(f"\nğŸ“ å¤„ç†ä½œå“: {title} (å…± {max_page} é¡µ)")

    for page_num in range(1, max_page + 1):
        # æ„å»ºé¡µé¢URL
        if page_num == 1:
            page_url = first_page_url
        else:
            page_url = first_page_url.replace('.html', f'-{page_num}.html')

        # è·å–å›¾ç‰‡URL
        img_url = get_image_url(page_url, session)
        if not img_url:
            print(f"âš ï¸ ç¬¬ {page_num} é¡µæ— æ³•è·å–å›¾ç‰‡URL")
            continue

        img_format = get_image_format(img_url)

        # âœ… å…³é”®ï¼šå…ˆæ£€æŸ¥è¿œç¨‹æ˜¯å¦å·²å­˜åœ¨
        if remote_file_exists(title, page_num, img_format):
            print(f"â­ï¸ {title} ç¬¬ {page_num} é¡µ å·²å­˜åœ¨ï¼Œè·³è¿‡")
            continue

        # ä¸‹è½½å›¾ç‰‡å†…å®¹ï¼ˆä¸ä¿å­˜åˆ°æœ¬åœ°ï¼Œç›´æ¥ä¸Šä¼ ï¼‰
        try:
            headers = {"User-Agent": random.choice(USER_AGENTS)}
            response = session.get(img_url, headers=headers, timeout=15)
            if response.status_code != 200:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {img_url} ({response.status_code})")
                continue

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶æˆ–ç›´æ¥ç”¨ BytesIOï¼ˆè¿™é‡Œç”¨ä¸´æ—¶æ–‡ä»¶æ›´ç®€å•ï¼‰
            temp_path = base_dir / f"temp_{page_num}.{img_format}"
            with open(temp_path, 'wb') as f:
                f.write(response.content)

            # ä¸Šä¼ åˆ°è¿œç¨‹
            if upload_to_remote(title, page_num, temp_path, img_format):
                pass  # ä¸Šä¼ æˆåŠŸ

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            temp_path.unlink(missing_ok=True)

            time.sleep(random.uniform(0.8, 1.8))

        except Exception as e:
            print(f"ğŸ’¥ ä¸‹è½½/ä¸Šä¼ ç¬¬ {page_num} é¡µå¤±è´¥: {e}")



def remote_file_exists(title, page, img_format):
    """æ£€æŸ¥è¿œç¨‹æœåŠ¡å™¨æ˜¯å¦å­˜åœ¨è¯¥æ–‡ä»¶"""
    try:
        url = f"{REMOTE_SERVER_URL}/exists"
        params = {'title': title, 'page': page, 'format': img_format}
        response = requests.get(url, params=params, timeout=5)
        if response.status_code == 200:
            return response.json().get('exists', False)
        else:
            print(f"[Remote Check] è¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"[Remote Check] å¼‚å¸¸: {e}")
        return False

def upload_to_remote(title, page, file_path, img_format):
    """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ°è¿œç¨‹æœåŠ¡å™¨"""
    try:
        with open(file_path, 'rb') as f:
            files = {'file': (f"{page}.{img_format}", f)}
            data = {'title': title, 'page': str(page)}
            response = requests.post(f"{REMOTE_SERVER_URL}/upload", data=data, files=files, timeout=30)
            if response.status_code == 200:
                print(f"âœ… å·²ä¸Šä¼  {title} ç¬¬ {page} é¡µ åˆ°è¿œç¨‹æœåŠ¡å™¨")
                return True
            else:
                print(f"âŒ ä¸Šä¼ å¤±è´¥: {response.text}")
                return False
    except Exception as e:
        print(f"âš ï¸ ä¸Šä¼ å¼‚å¸¸: {e}")
        return False
    

def clean_filename(filename):
    """ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡å­—ç¬¦"""
    # ç§»é™¤ä¸€äº›å±é™©çš„å­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡ã€ç©ºæ ¼ã€è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿
    cleaned = re.sub(r'[<>:"/\\|?*]', '', filename).strip()
    # é˜²æ­¢è·¯å¾„éå†æ”»å‡»
    cleaned = cleaned.replace('../', '').replace('..\\', '')
    return cleaned

if __name__ == "__main__":
    print("COSå†™çœŸä¸‹è½½å™¨ - å¼€å‘è€…ï¼šæš—ä¿¡æ‰“ä¸­å•")
    print("=" * 50)
    print("æ³¨æ„ï¼šæ­¤ç¨‹åºä¼šå¿½ç•¥SSLè¯ä¹¦éªŒè¯ä»¥è§£å†³CDNé—®é¢˜")
    print("=" * 50)
    download_cos_photos()